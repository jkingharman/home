title: human error
date: 17-11-2016

"Nobody knows anything" -- so said William Goldman, the [screenwriter](https://www.amazon.com/exec/obidos/ASIN/0446391174/thebigpictu09-20) and [novelist](https://en.wikipedia.org/wiki/William_Goldman).

Goldman was talking about Hollywood's [inability](https://www.economist.com/news/business/21591881-business-tinseltown-unpredictable-it-was-30-years-ago-even-now-nobody-knows) to [predict](https://www.amazon.co.uk/Hollywood-Economics-Uncertainty-Routledge-Contemporary/dp/0415312612) commercial success. But his line here is well known because he needn't have been: the same thing could be said with equal force in many other industries.

It is no secret that most unit trust [managers](https://www.bloomberg.com/view/articles/2014-05-21/hedge-funds-won-t-make-you-rich) [under](https://www.investorschronicle.co.uk/2015/11/24/comment/chris-dillow/another-mis-selling-scandal-zfwU3Ro1QD674rqOTnZlRP/article.html)-[perform](https://www.vanguard.co.uk/documents/adv/literature/case-for-index-fund-investing-uk-adviser-brief.pdf). Or that [political](https://repository.upenn.edu/cgi/viewcontent.cgi?article=1056&context=marketing_papers) [pundits's](https://www.newyorker.com/magazine/2005/12/05/everybodys-an-expert) [forecasts](https://www.project-syndicate.org/commentary/how-accurate-are-your-pet-pundits?barrier=true)[](https://www.newyorker.com/magazine/2005/12/05/everybodys-an-expert) are rarely better than chance. Or that consultancies's economic predictions are [often questionable](https://ritholtz.com/2016/04/161250/), for instance. The evidence that some professionals frequently overstate their skill and knowledge is overwhelming.

This raises a question: why is it that even smart people overestimate their knowledge of social life?

One basic reason is just the social world is tough to know -- often much tougher than the natural world.

Many spheres of social life, such as markets or governments, aren't orderly as clocks or tides are. They are rather examples of what Philip Tetlock calls "cloud-like" domains: messy systems that have many interrelated variables, exogenous factors and [reflexive](https://en.wikipedia.org/wiki/Positive_feedback#In_psychology) agents. It can be hard to establish strong correlative relationships, much less casual ones, in such domains. And hence the predictive power we want from science often evades us.

Take business growth for example. Common sense says factors like innovation, financial performance and managerial traits will be important here. But actually, they're maybe not. As [Alex Coad](ftp://papers.econ.mpg.de/evo/discussionpapers/2007-03.pdf) shows, these factors have only a weak link with growth. And studies haven't found stronger ones. Firm growth may well approximate a [random walk](https://en.wikipedia.org/wiki/Random_walk) -- which can explain why venture capitalist performance [varies](https://www.trustnet.com/venture-capital-trusts/price-performance?univ=VCT&Pf_sortedColumn=NP60M,UnitNameFull&Pf_sortedDirection=DESC) so much.

But there's more here.

The intrinsic difficulty of understanding social life is a big reason we overstate our knowledge in this sphere. Doubtless. Yet the fact that we are [deeply flawed knowers](https://en.wikipedia.org/wiki/Bounded_rationality) also matters a lot too.

As Daniel Kahneman [describes](https://www2.econ.iastate.edu/tesfatsi/JudgementAndChoice.MappingBoundedRationality.DKahneman2003.pdf), human minds are biased minds. Our cognition is governed by [adaptive biases](https://en.wikipedia.org/wiki/Adaptive_bias) that, though beneficial in some circumstances, dispose us to false beliefs and [irrational thought](https://en.wikipedia.org/wiki/List_of_cognitive_biases).

Many of these biases warp our thinking about social life especially. We are biased to find [causes](https://en.wikipedia.org/wiki/Magical_thinking) where [none](https://en.wikipedia.org/wiki/Illusion_of_control) exist (so get fooled by randomness, as Nassim Taleb [shows](https://en.wikipedia.org/wiki/Fooled_by_Randomness)). We are chronically [overconfident](https://en.wikipedia.org/wiki/Overconfidence_effect) (so go to [war](https://www.hup.harvard.edu/catalog.php?isbn=9780674015760), [sue](https://healy.econ.ohio-state.edu/papers/Moore_Healy-TroubleWithOverconfidence_WP.pdf) and [open shop](https://www.nesta.org.uk/sites/default/files/optimism_and_entrepreneurship_-_a_double-edged_sword.pdf) way too [often](https://www.jstor.org/stable/116990?seq=1#page_scan_tab_contents)). We [misremember](https://en.wikipedia.org/wiki/Hindsight_bias) past beliefs as being more accurate than they actually were (so rarely learn from mistakes). We try to [confirm](https://en.wikipedia.org/wiki/Confirmation_bias) rather than challenge our views (so are too quick to make up our minds and too slow to change them).

[And so on, alas](https://en.wikipedia.org/wiki/List_of_cognitive_biases). Behind all these specific errors is far broader [problem](https://medium.com/@davisdulin/defining-confidence-by-coherence-b8c1171b8197#.ybqbq9fg9). It's that our confidence in our beliefs is largely insensitive to amount and quality of evidence. What matters is just the *coherence* of the story we tell for our view. We rarely ask: am I missing something that might refute my belief? [WYSIATI](https://www.apa.org/monitor/2012/02/conclusions.aspx).

Which brings us back to the overconfidence of pundits, fund managers and others.

That such professionals often know less than they think doesn't bother me: knowing [your competency's edge](https://www.farnamstreetblog.com/2013/12/mental-model-circle-of-competence/) is hard. What bothers is that, in acting so confidently, they sometimes legitimise bad ideas -- like that the future is knowable with precision; that risk can always be managed away; and that countries are highly controllable.

Ideas like these partly enabled the Iraq war and financial crisis and are still widely held. It's the same old stupidity. And we won't learn until free admissions of ignorance are seen for what they are: not weakness, but the beginnings of wisdom.
