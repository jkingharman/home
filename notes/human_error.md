title: Human error
date: 2017

 
"Nobody knows anything" so said William Goldman, the screenwriter and novelist.
 
Goldman was talking about Hollywood's inability to predict commercial success. But his line here is well known because he needn't have been: the same thing could be said with equal force in many other industries.
 
It is no secret that most unit trust managers under-perform. Or that political pundits's forecasts are rarely better than chance. Or that consultancies's economic predictions are often worthless, for instance. The evidence that some professionals frequently overstate their skill and knowledge is overwhelming.
 
This raises a question: why is it that even smart people overestimate their knowledge of the social life?
 
One basic reason is just that social world is tough to know â€“ often much tougher than the natural world.
 
Many spheres of social life, such as markets or governments, aren't orderly as clocks or tides are. They are rather examples of what Philip Tetlock calls cloud-like domains: messy systems that have many interrelated variables, exogenous factors, and reflexive agents. It can be hard to establish strong correlative relationships, much less casual ones, in such domains. And hence the predictive power we want from science often evades us.
 
Take business growth for example. Common sense says factors like innovation, financial performance and managerial traits will be important here. But actually, they're not. As Alex Coad shows, these factors have only a weak link with growth. And studies haven't found stronger ones. Firm growth may well approximate a random walk - which can explain why venture capitalist performance varies so much.
 
But there's more here.
 
The intrinsic difficulty of understanding social life is a big reason we overstate our knowledge in this sphere. Doubtless. Yet the fact that we are deeply flawed knowers also matters a lot too.
 
As Daniel Kahneman describes, human minds are biased minds. Our cognition is governed by adaptive biases that, though beneficial in some circumstances, dispose us to false beliefs and irrational thought.
 
Many of these biases warp our thinking about social life especially. We are biased to find causes where none exist (so get fooled by randomness, as Nassim Taleb shows). We are chronically overconfident (so go to war, sue and open shop way too often). We misremember past beliefs as being more accurate than they actually were (so rarely learn from mistakes). We try to confirm rather than challenge our views (so are too quick to make up our minds and too slow to change them).
 
And so on alas. Behind all these specific errors is far broader problem. It's that our confidence in our beliefs is largely insensitive to amount and quality of evidence. What matters is just the coherence of the story we tell for our view. We rarely ask: am I missing something that might refute my belief? WYSIATI.
 
Which brings us back to the overconfidence of pundits, fund managers, and others.
 
That such professionals often know less than they think doesn't bother me: knowing your competency's edge is hard. What bothers is that, in acting so confidently, they legitimise bad ideas - like that the future is knowable with precision; that risk can always be managed away; and that countries are highly controllable.
 
Ideas like these partly enabled the Iraq war and financial crisis, and are still widely held. It's the same old stupidity. And we won't learn until free admissions of ignorance are seen for what they are: not weakness, but the beginnings of wisdom.
